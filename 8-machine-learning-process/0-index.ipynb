{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last lesson, we saw how we can correct for our different sources of errors.  We know that in training our models, our models will train to randomness of the data when given a chance.  To correct for this, we have a holdout set and choose the complexity of the model based on how the model scores on these holdout sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's review this and show one last component to evaluating our machine learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. We load our feature data sets.  \n",
    "\n",
    "Now in this lesson we're having each of our feature datasets be of length 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import input_temps, temps_and_is_weekends, temps_weekends_and_ages, customers_with_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_datasets = [input_temps, temps_and_is_weekends, temps_weekends_and_ages]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[150, 150, 150]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(map(lambda dataset: len(dataset), feature_datasets))\n",
    "# [100, 100, 100]\n",
    "# len(customers_with_errors)\n",
    "# 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Then we split these datasets into training and holdout sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_datasets = []\n",
    "for dataset in feature_datasets:\n",
    "    training_data = dataset[:100]\n",
    "    holdout_data = dataset[100:]\n",
    "    split_dataset = (training_data, holdout_data)\n",
    "    split_datasets.append(split_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. We train each model on it's respective training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False),\n",
       " LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False),\n",
       " LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "training_models = []\n",
    "for dataset in split_datasets:\n",
    "    model = LinearRegression()\n",
    "    training_feature_data = dataset[0]\n",
    "    model.fit(training_feature_data, customers_with_errors[0:100])\n",
    "    training_models.append(model)\n",
    "training_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([3.12480048]),\n",
       " array([ 3.11082792, 45.65739545]),\n",
       " array([ 3.09483918, 47.24264466,  0.45487237])]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(map(lambda training_model: training_model.coef_, training_models))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. We evaluate our models on the training and holdout datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_models_with_data = list(zip(training_models, split_datasets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_scores = []\n",
    "for split_model_with_data in split_models_with_data:\n",
    "    selected_model = split_model_with_data[0]\n",
    "    training_data = split_model_with_data[1][0]\n",
    "    training_score = selected_model.score(training_data, customers_with_errors[:100])\n",
    "    training_scores.append(training_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "holdout_scores = []\n",
    "for split_model_with_data in split_models_with_data:\n",
    "    selected_model = split_model_with_data[0]\n",
    "    holdout_data = split_model_with_data[1][1]\n",
    "    holdout_score = selected_model.score(holdout_data, customers_with_errors[100:])\n",
    "    holdout_scores.append(holdout_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8924818514798912, 0.9457660708435309, 0.9491031227430928]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6179093480262522, 0.7780004499162101, 0.7689173517214827]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "holdout_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then after looking at our training and holdout scores, we choose the model where the holdout score peaks, which is our second model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| model                  |train score | holdout score |  \n",
    "| ---------------------- |:----------:| :------------:|\n",
    "| temps                  | .89        |  .61 | \n",
    "| temps, weekend         | .945        |  .778 \n",
    "| temps, ages, weekend   | .949        |  .768         |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Just a little bit more"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we choose our second model as it scores the best, and place it into production.  Temperatures and weekend data be our best combination of parameters in predicting the amount of customers.  And then we see that this our holdout score tends to overperform how our model performs in production."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This may be surprising, but it is yet another case of overfitting.  In the first case, where we trained our models, we risked overfitting because our model can simply find numbers that match the randomness in the data.  In the second case, where we tune our model by choosing different amounts of features, and as we'll see, make other modifications to our model, the exact tuning of our model may be overfit to our holdout set.  Tuning and tweaking our model can take hours, so the phenomena where we tune parameters that just randomly happen to best fit our holdout set is not uncommon.\n",
    "\n",
    "Because of this, data scientists segment their datasets into three group: a training set, a validation set and a test set.  \n",
    "* The training set\n",
    "The training set is used to train parameters on with the different variations of the model.  Because these *parameter values* are subject to overfitting against the training set, especially as the flexibility in our model increases, we use a validation set to protect against this.  \n",
    "\n",
    "* The validation set\n",
    "After the model is trained we look at the scores of our validation set to choose the tuning parameters in our model -- here our tuning parameter is just the type of features we include.  However, as we learn how to tweak our models more and more, we run the risk of tweaking parameters that just happen to work well with our validation set, but won't perform as well in production.\n",
    "\n",
    "* The test set\n",
    "Because of this, after we select the parameters, we run this model against another holdout set of data we have not yet used, called our test set.  Because our test set was not used to train our models, nor to choose tuning parameters, we expect the score of the test set to most approximate how our model performs in production.  We also can use our test set to compare performance against other machine learning algorithms, like comparing the performance of our linear regression model against the performance of our random forests model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training, Validation and Tests in Action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now let's go through our machine learning process again, but this time,Â segmenting our dataset into training data, validation data, and testing data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have our same `feature_datasets` of one parameter, two parameter, and three parameter data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_validate_test_datasets = []\n",
    "for dataset in feature_datasets:\n",
    "    training_data = dataset[:90]\n",
    "    validation_data = dataset[90:120]\n",
    "    test_data = dataset[120:]\n",
    "    train_validate_test_dataset = (training_data, validation_data, test_data)\n",
    "    train_validate_test_datasets.append(train_validate_test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that our first dataset with one parameter is split into groups of three."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_validate_test_datasets[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can train each of our models with the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False),\n",
       " LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False),\n",
       " LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "updated_training_models = []\n",
    "for dataset in train_validate_test_datasets:\n",
    "    model = LinearRegression()\n",
    "    training_feature_data = dataset[0]\n",
    "    model.fit(training_feature_data, customers_with_errors[0:90])\n",
    "    updated_training_models.append(model)\n",
    "updated_training_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([3.15452937]),\n",
       " array([ 3.14877452, 46.60849399]),\n",
       " array([ 3.11976833, 48.8545893 ,  0.4403944 ])]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(map(lambda model: model.coef_, updated_training_models))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can evaluate the models with the validation sets to select the number of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_with_trisplits = list(zip(updated_training_models, train_validate_test_datasets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_scores = []\n",
    "for trisplit_model_with_data in models_with_trisplits:\n",
    "    selected_model = trisplit_model_with_data[0]\n",
    "    validation_data = trisplit_model_with_data[1][1]\n",
    "    validation_score = selected_model.score(validation_data, customers_with_errors[90:120])\n",
    "    validation_scores.append(validation_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.14886339953110272, 0.4943257519541469, 0.5002740763480484]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally we select the second model, and then choose this model to calculate the test score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_scores = []\n",
    "for trisplit_model_with_data in models_with_trisplits:\n",
    "    selected_model = trisplit_model_with_data[0]\n",
    "    test_data = trisplit_model_with_data[1][2]\n",
    "    test_score = selected_model.score(test_data, customers_with_errors[120:])\n",
    "    test_scores.append(test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.16574724727928036, 0.47415508688958946, 0.49213683932072705]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We expect the numbers in our test scores to better approximate how our model will perform in production."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
