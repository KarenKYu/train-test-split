{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last lesson, we saw how we can correct for our different sources of errors.  We know that in training our models, our models will train to randomness of the data when given a chance.  To correct for this, we have a holdout set and choose the complexity of the model based on how the model scores on these holdout sets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's review this and show one last component to evaluating our machine learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| model                  |train score | holdout score |  \n",
    "| ---------------------- |:----------:| :------------:|\n",
    "| temps, ages, weekend   | .948        |  .73          |\n",
    "| temps, weekend         | .942        |  .78 \n",
    "| temps                  | .78        |  .47 | "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now here we are making one tweak to our model, whether or not to include the `random_ages` feature.  However, you could imagine that there are many other features that we decide whether or not to include in our model.  So we prevent training against them, but with enough tweaks to the model, we are likely to get a tweak that randomly improves the model against the validation set.  \n",
    "\n",
    "Because of this, data scientists still are susceptible to seeing their models perform better against the validation set than in production.  So we have another solution for this: the test set.  The test set is yet a second holdout set to evaluate the performance of our machine learning model.  This allows us to get a better sense of how well our model will perform in production."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[validation sets](https://datascience.stackexchange.com/questions/18339/why-use-both-validation-set-and-test-set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
