{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time to try it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, let's try it.  We'll create holdout sets by splitting our data into a group of holdout training data and holdout data.  So let's first train the three models on the training datasets.  We'll do this by selecting roughly two thirds of the data to train our model on and use the rest of the data for the holdout set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_models = []\n",
    "training_predictions = []\n",
    "for feature_data in feature_datasets:\n",
    "    model = LinearRegression()\n",
    "    model.fit(feature_data[0:30], customers_with_errors[0:30])\n",
    "    trained_models.append(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can see how well our feature data fits to the data that we trained on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_predictions = []\n",
    "for feature_data, trained_model in zip(feature_datasets, trained_models):\n",
    "    training_predictions.append(trained_model.predict(feature_data[0:30]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[28.123394526498657, 20.233133908472176, 19.11337786070657]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from numpy import sqrt\n",
    "training_rmses = [sqrt(mean_squared_error(training_prediction, customers_with_errors[0:30])) for training_prediction in training_predictions]\n",
    "training_rmses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So as we can see on our training data, our training errors decrease with every new parameter that we add.  Now let's see how each of our trained models performs with our holdout data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "holdout_predictions = []\n",
    "for feature_data, trained_model in zip(feature_datasets, trained_models):\n",
    "    holdout_predictions.append(trained_model.predict(feature_data[30:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[27.73131229156565, 24.182266312981024, 28.926874485322074]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from numpy import sqrt\n",
    "holdout_rmses = [sqrt(mean_squared_error(holdout_prediction, customers_with_errors[30:])) for holdout_prediction in holdout_predictions]\n",
    "holdout_rmses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So here we can see that our models do not improve with each added parameter.  Rather we see that our bias variance tradeoff reflected in these scores.  The first score is for our one parameter model, which is biased.  Then the second score has our two parameter model, which has the proper features of temps and weekend.  And our final model has three an irrelevant parameter of `random_ages` and thus suffers from variance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| model                  |train rmse | holdout rmse |  \n",
    "| ---------------------- |:---------:| :------------:|\n",
    "| temps, ages, weekend   | 28.12     |  27.73        |\n",
    "| temps, weekend         | 20.23     |  24.18        |\n",
    "| temps                  | 19.13     |  28.92         | "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from graph import trace_values\n",
    "# model_parameters = [1, 2, 3]\n",
    "# training_trace = trace_values(model_parameters, training_rmses, mode = 'lines', name = 'training')\n",
    "# holdout_trace = trace_values(model_parameters, holdout_rmses, mode = 'lines', name = 'holdout')\n",
    "# plot([holdout_trace, training_trace])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above is a typical graph in machine learning.  As we add more features, the training scores tend to continue to improve.  However, at a certain point adding too many features has our model suffer from variance, so our holdout scores do not continue to improve.  The peak of our holdout scores is showing our best version of the model, which is the second model.\n",
    "\n",
    "So this technique can help us toanswer when our models suffer from bias and when they suffer from variance.  \n",
    "* In the first model, where we are only training on temperatures, our model suffers from bias, as it does not capture the variation that comes from being a weekend.  \n",
    "* In the third model, we suffer from overfitting, which increases our error due to variance.  By adding an irrelevant feature such as age of the cashier to train on, our model's error decreases on the training set.  But this is just the model adjusting to randomness, which we see because our model does not generalize to future data. \n",
    "* The second model, where the error of our holdout set reaches the lowest point, is the model we choose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lesson, we learned about the bias variance tradeoff.  There are two main charts that we saw.  The first is that as we add more features, our models become more flexible, and are able to capture the variation in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But the second shows some of the variation in the data is likely randomness.  We see that as we add more features, the error on our training dataset continues to reduce.  However, at a certain point these improved scores on our training set are due to the models fitting to randomness in the data that is unlikely to generalize.  We see this, when by adding more parameters the performance on the holdout set gets worse.  This displays overfitting to the training set.  To combat against overfitting, we choose the model that performs best on the holdout set."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
