{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The bias variance tradeoff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now been exposed to the three different sources of error that can arise in our model.  Here are the three sources:\n",
    "\n",
    "* **bias** when our model does not include one or more features that contributes to variations in outcomes of our data.  \n",
    "* **variance** when we include too many features in our model that make our models too flexible, such that it picks up on randomness in the data.  \n",
    "* **irreducible error** which occurs due to a degree of randomness in our target variable. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we know, we cannot eliminate irreducible error.  But we can develop techniques for not including too few parameters and thus contributing bias, or too many parameters and thus introducing variance, into our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up our data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we have seen a model that suffers from bias, one from variance, and another that has the correct number of variables.  In this lesson, we'll take a look at each of these types of models togther and see how they compare.\n",
    "\n",
    "Now to create these three models, we'll just use the same process of inititializing our model, fitting the data and looking at some scores.  The only difference between the models will be the features that we pass into the models.  Lucky for us, we already have these features loaded in a separate file.  \n",
    "\n",
    "So let's load up the data, and then fit three different models with the following features:\n",
    "\n",
    "1. temperatures,\n",
    "2. temperatures and weekends, and\n",
    "3. temperatures, weekends and ages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([2.87988515]),\n",
       " array([ 3.07299452, 38.61313304]),\n",
       " array([ 3.07698899, 38.62306381, -0.05584566])]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from data import input_temps, temps_and_is_weekends, temps_weekends_and_ages, customers_with_errors\n",
    "feature_datasets = [input_temps, temps_and_is_weekends, temps_weekends_and_ages]\n",
    "models = []\n",
    "for dataset in feature_datasets:\n",
    "    model = LinearRegression()\n",
    "    model.fit(dataset, customers_with_errors)\n",
    "    models.append(model)\n",
    "models\n",
    "\n",
    "intercepts = [model.intercept_ for model in models]\n",
    "# [35.62031572335471, 9.854773197812762, 12.155548281106803]\n",
    "\n",
    "coefs = [model.coef_ for model in models]\n",
    "coefs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, let's take a look at these models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://plot.ly/~JeffKatzy/247'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import plotly.plotly as py\n",
    "from data import data_trace, prediction_traces\n",
    "from graph import plot\n",
    "py.plot([data_trace,*prediction_traces])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Now it may look like there are only two models, but there are three, and they are different.  Hover your cursor over the plot and you can see the slight differences between our two feature and three feature models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's move into our process for choosing a model.  The first thing we may want to do is look at the scores for each of these models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[27.945349336535738, 21.565735597602885, 21.556603670493452]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "[sqrt(mean_squared_error(customers_with_errors,model.predict(dataset))) for model, dataset in zip(models, feature_datasets)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see that each of has progressively better scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, as we mentioned previously the new feature in our last model is the influence of our random list of cashier ages to customers.  This model is just using this random list to find an association between cashier age and customers that doesn't exist.  It's simply coincidence that there exists this association.  This is another case of us overfitting to the randomness in the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### But how do we know?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the way to detect this overfitting is to use a holdout set.  Here's how we do this:\n",
    "\n",
    "1. We separate our model into two groups.  \n",
    "    * A random selection of 80 percent of the data, and then the remaining 20 percent of the data.  \n",
    "2. We'll then train our data on just the 80 percent of the data\n",
    "3. Finally, we see how well our model performs on the remaining 20 percent of the data that was not a part of the training process.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's why this should work:\n",
    "\n",
    "1. **If the association is real**, then the detected association should continue to be informative even on data the our model did not see.  \n",
    "2. **If the association is not real** then this means that our model just found a random association that is not likely to be replicated on the holdout set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time to try it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, let's try it.  We'll create holdout sets by splitting our data into a group of holdout training data and holdout data.  So let's first train the three models on the training datasets.  We'll do this by selecting roughly two thirds of the data to train our model on and use the rest of the data for the holdout set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_models = []\n",
    "training_predictions = []\n",
    "for feature_data in feature_datasets:\n",
    "    model = LinearRegression()\n",
    "    model.fit(feature_data[0:30], customers_with_errors[0:30])\n",
    "    trained_models.append(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can see how well our feature data fits to the data that we trained on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_predictions = []\n",
    "for feature_data, trained_model in zip(feature_datasets, trained_models):\n",
    "    training_predictions.append(trained_model.predict(feature_data[0:30]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[28.123394526498657, 20.233133908472176, 19.11337786070657]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from numpy import sqrt\n",
    "training_rmses = [sqrt(mean_squared_error(training_prediction, customers_with_errors[0:30])) for training_prediction in training_predictions]\n",
    "training_rmses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So as we can see on our training data, our training errors decrease with every new parameter that we add.  Now let's see how each of our trained models performs with our holdout data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "holdout_predictions = []\n",
    "for feature_data, trained_model in zip(feature_datasets, trained_models):\n",
    "    holdout_predictions.append(trained_model.predict(feature_data[30:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[27.73131229156565, 24.182266312981024, 28.926874485322074]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from numpy import sqrt\n",
    "holdout_rmses = [sqrt(mean_squared_error(holdout_prediction, customers_with_errors[30:])) for holdout_prediction in holdout_predictions]\n",
    "holdout_rmses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So here we can see that our models do not improve with each added parameter.  Rather we see that our bias variance tradeoff reflected in these scores.  The first score is for our one parameter model, which is biased.  Then the second score has our two parameter model, which has the proper features of temps and weekend.  And our final model has three an irrelevant parameter of `random_ages` and thus suffers from variance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| model                  |train rmse | holdout rmse |  \n",
    "| ---------------------- |:---------:| :------------:|\n",
    "| temps, ages, weekend   | 28.12     |  27.73        |\n",
    "| temps, weekend         | 20.23     |  24.18        |\n",
    "| temps                  | 19.13     |  28.92         | "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://plot.ly/~JeffKatzy/245'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from graph import trace_values\n",
    "import plotly.plotly as py\n",
    "model_parameters = [1, 2, 3]\n",
    "training_trace = trace_values(model_parameters, training_rmses, mode = 'lines', name = 'training')\n",
    "holdout_trace = trace_values(model_parameters, holdout_rmses, mode = 'lines', name = 'holdout')\n",
    "py.plot([holdout_trace, training_trace])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above is a typical graph in machine learning.  As we add more features, the training scores tend to continue to improve.  However, at a certain point adding too many features has our model suffer from variance, so our holdout scores do not continue to improve.  The peak of our holdout scores is showing our best version of the model, which is the second model.\n",
    "\n",
    "So this technique can help us toanswer when our models suffer from bias and when they suffer from variance.  \n",
    "* In the first model, where we are only training on temperatures, our model suffers from bias, as it does not capture the variation that comes from being a weekend.  \n",
    "* In the third model, we suffer from overfitting, which increases our error due to variance.  By adding an irrelevant feature such as age of the cashier to train on, our model's error decreases on the training set.  But this is just the model adjusting to randomness, which we see because our model does not generalize to future data. \n",
    "* The second model, where the error of our holdout set reaches the lowest point, is the model we choose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lesson, we learned about the bias variance tradeoff.  There are two main charts that we saw.  The first is that as we add more features, our models become more flexible, and are able to capture the variation in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But the second shows some of the variation in the data is likely randomness.  We see that as we add more features, the error on our training dataset continues to reduce.  However, at a certain point these improved scores on our training set are due to the models fitting to randomness in the data that is unlikely to generalize.  We see this, when by adding more parameters the performance on the holdout set gets worse.  This displays overfitting to the training set.  To combat against overfitting, we choose the model that performs best on the holdout set."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
