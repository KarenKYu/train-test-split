{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In previous lessons we learned why we split our data into a training and test set.  The training set is used to fit the model and the test set is used to evaluate how well the model performed.  It turns our we should really split our data one additional time, so that we have a training, validation, and then a test set.  \n",
    "\n",
    "In this lesson we'll learn why we split into three groups and what each group is responsible for."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up our data and models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this lesson, we'll use the boston dataset.  We start by splitting our data into two groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "dataset = load_boston()\n",
    "\n",
    "X = dataset['data']\n",
    "y = dataset['target']\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then we train our model on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None,\n",
       "         normalize=False)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review: Why split into two groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, if we look at the accuracy of the model, we see that our $r^2$ score is higher on the training set than on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7419034960343789"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7147895265576858"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is expected.  It occurs because when we train our model, the model simply tries to fit to the training data.  The training data has a degree of randomness in it, and the model fits to the randomness in the training data as well as the underlying trend.  But because we do not expect to occur again, our model tends to match training data better than the holdout data it hasn't seen. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now this seems like a good strategy.  But then comes the task ofÂ selecting features for our model.  In this dataset, we have thirteen different features, but in many datasets we will have hundreds of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(506, 13)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we select features for our model, we'll do so by trying models with different collections of features, and evaluate how well they perform on the test set.  Then, we'll select a model by generally choosing the model that performs best on the test set.\n",
    "\n",
    "Now if we use this model to predict future data, will it perform as well as it did on the test set?  Probably not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem is that when selecting our features, we may be trying hundreds if not thousands of different models.  And then, by evaluating a model based on how it performs on the test set, we may be selecting a model that just happens to match up to the randomness in the data of the test set.  And of course we don't expect to see that randomness again in future data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our remedy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can fix this by splitting our data into three groups.  A training set, a validation set, and a test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Training set** We use the training set to fit each of the models.\n",
    "* **Validation Set**  We use the validation set to see how well each model performs on data it has not yet seen.\n",
    "* **Test Set**  Because we want to avoid selecting a model that happens to perform well on the validation set due to random chance, when we are done choosing our model, we evaluate our final model on the test set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performing this operation is simple enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "dataset = load_boston()\n",
    "\n",
    "X = dataset['data']\n",
    "y = dataset['target']\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.4, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can see that we started with a dataset of the following shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(506, 13)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we have split our dataset into the following group sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(102, 13)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape\n",
    "# (354, 13)\n",
    "\n",
    "X_val.shape\n",
    "# (81, 13)\n",
    "\n",
    "X_test.shape\n",
    "# (152, 13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have segmented our data.  The ideal move is to store our test data in it's own file, and never look at this file until we are ready to evaluate our final model.  We can do the same from our training and validation sets so that we don't use any data that is in our test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train_df = pd.DataFrame(X_train, columns = dataset['feature_names'])\n",
    "train_df.loc[:, 'y'] = y_train\n",
    "train_df.to_feather('./train.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_df = pd.DataFrame(X_val, columns = dataset['feature_names'])\n",
    "validate_df.loc[:, 'y'] = y_val\n",
    "validate_df.to_feather('./validate.feather')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We can also choose to keep the training and validation sets combined, and later split them.  The key is just to have one set of data -- our test set -- that we store in a separate file and do not look at until the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.DataFrame(X_test, columns = dataset['feature_names'])\n",
    "test_df.loc[:, 'y'] = y_test\n",
    "test_df.to_feather('./test.feather')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lesson, we learned about an issue that can occur when developing a machine learning model.  We do so by training multiple different models and evaluating how each performs on a holdout set.  With each model that we attempt and then evaluate based on the holdout set, we increase the chance of finding a model that performs well because it matches the randomness in the holdout set data.  To correct for this, we split our data into three groups.  The first group is for training, the second group is for comparing models, and the third set is for evaluating how well our chosen model will perform on data it (nor us) have previously seen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Slate Gelman](https://slate.com/technology/2013/07/statistics-and-psychology-multiple-comparisons-give-spurious-results.html)\n",
    "\n",
    "[Garden of Forking Paths](https://www.americanscientist.org/article/the-statistical-crisis-in-science#)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
