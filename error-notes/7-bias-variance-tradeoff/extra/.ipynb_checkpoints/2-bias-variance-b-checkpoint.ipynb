{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's move into our process for choosing a model.  The first thing we may want to do is look at the scores for each of these models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[27.945349336535738, 21.565735597602885, 21.556603670493452]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "[sqrt(mean_squared_error(customers_with_errors,model.predict(dataset))) for model, dataset in zip(models, feature_datasets)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see that each of has progressively better scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, as we mentioned previously the new feature in our last model is the influence of our random list of cashier ages to customers.  This model is just using this random list to find an association between cashier age and customers that doesn't exist.  It's simply coincidence that there exists this association.  This is another case of us overfitting to the randomness in the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### But how do we know?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the way to detect this overfitting is to use a holdout set.  Here's how we do this:\n",
    "\n",
    "1. We separate our model into two groups.  \n",
    "    * A random selection of 80 percent of the data, and then the remaining 20 percent of the data.  \n",
    "2. We'll then train our data on just the 80 percent of the data\n",
    "3. Finally, we see how well our model performs on the remaining 20 percent of the data that was not a part of the training process.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's why this should work:\n",
    "\n",
    "1. **If the association is real**, then the detected association should continue to be informative even on data the our model did not see.  \n",
    "2. **If the association is not real** then this means that our model just found a random association that is not likely to be replicated on the holdout set."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
